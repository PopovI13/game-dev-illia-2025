<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <title>t1a15-liteRTjs-illia</title>
  <style>
    body { font-family: Arial; background: #f4f4f4; padding: 20px; }
    h1 { text-align: center; }
    #output {
      background: white;
      padding: 15px;
      border-radius: 8px;
      box-shadow: 0 0 5px rgba(0,0,0,0.1);
      white-space: pre-wrap;
    }
  </style>
</head>
<body>
  <h1>t1a15-liteRTjs-illia Demo</h1>
  <p>This page automatically loads a TensorFlow Lite model from a URL.</p>
  <div id="output">Loading model...</div>

  <script type="module">
    import * as tflite from "https://cdn.jsdelivr.net/npm/@tensorflow/tfjs-tflite";

    const outputEl = document.getElementById("output");

    async function loadAndRunModel() {
      try {
        // üëá Replace this with the actual path to your model
        const modelUrl = "https://hpaajellis.github.io/my-examples-of-liteRTjs/public/model.tflite";

        outputEl.textContent = "Fetching model from: " + modelUrl;
        const response = await fetch(modelUrl);
        if (!response.ok) throw new Error("Model file not found at " + modelUrl);
        const buffer = await response.arrayBuffer();

        outputEl.textContent = "Loading model into liteRT.js...";
        const model = await tflite.loadTFLiteModel(buffer);
        outputEl.textContent = "‚úÖ Model loaded!\nRunning inference...";

        // Example input (you can change this based on your model)
        const input = new Float32Array([0.1, 0.2, 0.3]);
        const output = model.predict(input);
        outputEl.textContent = "‚úÖ Inference done!\n\nResult:\n" + JSON.stringify(output, null, 2);
      } catch (err) {
        outputEl.textContent = "‚ùå Error: " + err.message;
      }
    }

    loadAndRunModel();
  </script>
</body>
</html>
